{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "82b8dfab-bdd0-4f2b-a6bc-66591817c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\sushrutha\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sushrutha\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sushrutha\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sushrutha\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82eefa5-4be1-472d-acf9-8297e92fa368",
   "metadata": {},
   "source": [
    "## 1). Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "866152f0-4b79-4753-aa7f-5a543f6a1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello My Name is Sushrutha.\n",
    "I Am Studying In RNSIT Bangalore.I Am Learning New Course! to enhance my practical skills in the in the industry.All the Very best.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "6f9bb56f-efe6-4b1d-86d7-c0a264b987ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello My Name is Sushrutha.\n",
      "I Am Studying In RNSIT Bangalore.I Am Learning New Course! to enhance my practical skills in the in the industry.All the Very best.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9d3f4-b918-4a4e-8beb-a649f44d1873",
   "metadata": {},
   "source": [
    "## Sentences --> Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "88c23b8a-d724-4b04-87b6-68b657fbea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "6e321712-789f-47d5-a185-2fa6793ede60",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "4e85088c-e92a-4655-b4cf-a136f13684a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello My Name is Sushrutha.\n",
      "I Am Studying In RNSIT Bangalore.I Am Learning New Course!\n",
      "to enhance my practical skills in the in the industry.All the Very best.\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000c54f-d7c8-4ed0-9bf5-c8faa95e4779",
   "metadata": {},
   "source": [
    "### Paragraph --> Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "56bd8bf8-8184-4f66-9f4d-217d0f22093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "01ffe6fe-d03f-4c61-9ceb-ef9e84a9e2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'My',\n",
       " 'Name',\n",
       " 'is',\n",
       " 'Sushrutha',\n",
       " '.',\n",
       " 'I',\n",
       " 'Am',\n",
       " 'Studying',\n",
       " 'In',\n",
       " 'RNSIT',\n",
       " 'Bangalore.I',\n",
       " 'Am',\n",
       " 'Learning',\n",
       " 'New',\n",
       " 'Course',\n",
       " '!',\n",
       " 'to',\n",
       " 'enhance',\n",
       " 'my',\n",
       " 'practical',\n",
       " 'skills',\n",
       " 'in',\n",
       " 'the',\n",
       " 'in',\n",
       " 'the',\n",
       " 'industry.All',\n",
       " 'the',\n",
       " 'Very',\n",
       " 'best',\n",
       " '.']"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "bb651a4d-264e-459b-b23b-55fd742ac4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'My', 'Name', 'is', 'Sushrutha', '.']\n",
      "['I', 'Am', 'Studying', 'In', 'RNSIT', 'Bangalore.I', 'Am', 'Learning', 'New', 'Course', '!']\n",
      "['to', 'enhance', 'my', 'practical', 'skills', 'in', 'the', 'in', 'the', 'industry.All', 'the', 'Very', 'best', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(word_tokenize(sentence)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "4b95b7f4-bed3-42f6-a5c3-254fc8cc3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it don' separates the full stop in the midle of the sentence , only include in the last\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "e6c49f63-98ad-4d66-835f-4303e3b582e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd9c31-7dfd-4e1f-bb7b-3d418e0e7f20",
   "metadata": {},
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c226a54-5818-4245-ac58-4a207df34e16",
   "metadata": {},
   "source": [
    "## 2). Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e74b5c-499f-4adf-bb7b-fcf6050f35ac",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "0f850130-eca6-4924-b0eb-90aa255fc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "4c7ed4fe-8f04-4992-9eb4-0c687b431565",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "a20dc31f-d2e9-43df-8aa6-5b7881245870",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating','eats','eaten','writing','writes','programming','history','finally','finallized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "d1add477-dec9-4431-849d-52ba7d21b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finallized----->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"----->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "72cae081-0204-4453-9546-34c0510c855d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "380887e7-8dbc-430d-8016-3d0a85811605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairli'"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "84eb64f7-b353-42a7-bee3-d52e8c9bab1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sportingli'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6bbac-af11-4dfc-90e0-42bfa2cf9f9a",
   "metadata": {},
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "f0b68e38-3805-490a-a3c4-03873ac26711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "9ff5e154-9dc9-43b4-b593-3b94500b793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whenever last word is 'ing','e','able' it removes that\n",
    "reg_stemmer = RegexpStemmer('ing$|e$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "fa0da71a-1eef-44a2-bd93-23662890901a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "affe8b46-dd7d-4e53-8d45-8cbe5a8d27ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec361a74-6561-4911-bdab-cea39c9a6870",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "92a4ca2b-8e02-4793-a5a5-b3254f2be278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "cc11376e-e5e0-44fa-bda7-761b7d83ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "54c90cf9-5e29-4fa4-949f-bee03b672b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('fairly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "b001e843-0dd5-4f67-bc03-5b69963f300b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sport'"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b9dee-0194-4477-a657-7fc4d51a0aa4",
   "metadata": {},
   "source": [
    "## 3). Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69101bdf-be7c-4990-b3d1-0427bdc9573d",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "e0443e8b-cdf4-4d90-93c4-46e1621936a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sushrutha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "a33727a0-ba63-47f0-993e-b73fa2b9211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "a83e64fd-fb90-4440-b08a-733ad52378e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "f13c8ea3-3f23-4c1b-87dd-e1536d2b3681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS- Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r\n",
    "'''\n",
    "lemmatizer.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "d176dde1-08fc-4886-af9a-0bece4e76e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating','eats','eaten','writing','writes','programming','history','finally','finallized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "038a84f6-6fe1-46be-a987-18ce3f23eb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "history---->history\n",
      "finally---->finally\n",
      "finallized---->finallized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa08b61-bfcd-4c0e-ba79-fb19e711f03b",
   "metadata": {},
   "source": [
    "## 4). StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "3dc81b93-e1d7-49bc-9c53-bf0c58e27324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sushrutha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "5fab4f0f-a06a-435d-af75-779b9da78998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "b37161b7-9261-465c-8168-dddf23e3277b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "60b08210-e1ae-42de-960a-77059c3a9149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "9d1477dd-18f4-44da-953b-76b7d3fe3068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "bd05f2e9-00d5-4d08-8d6c-7ca9bcb04034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "2c81640f-fe88-4562-9f0d-213cea190b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "c1bca884-2a05-4b73-8aaa-b8c8d4b344e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cda8c49-2f48-4529-b99e-3bb2c25eee82",
   "metadata": {},
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "182b1445-2fb1-44b5-8652-ff24677855eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "a068b547-ce76-4511-8436-e453805385b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for India’s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "6b79d5ea-1965-4fe6-b0e3-0622a077f407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "e6d34cc9-7528-4f00-b178-1db7728b0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PorterStemmer\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])  # Tokenize the sentence into words\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # applying stopwords for words\n",
    "    sentences[i] = ' '.join(words)  # Convert the list of words back into a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "ab8ce725-5e8a-424a-ab28-331f86045c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "8900da25-5e36-4493-8600-105265241bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "77668874-54bd-4f78-80e9-c5de7ab8162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Snowball Stemmer\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])  # Tokenize the sentence into words\n",
    "    words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]  \n",
    "    sentences[i] = ' '.join(words)  # Convert the list of words back into a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "b0c7022e-e487-4ade-9d84-7d25760943d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'on strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "02ce6486-5bdc-4483-9e31-1e1f3b64cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "34d1b55a-3a51-4c4a-a0ab-59110513f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])  # Tokenize the sentence into words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]  \n",
    "    sentences[i] = ' '.join(words)  # Convert the list of words back into a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "14d42e0f-67c4-4754-b940-0b152b8bded8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad u , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot u , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect u .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect u .',\n",
       " 'strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973b638-e1e8-4e0f-b88d-550ecc3a52ac",
   "metadata": {},
   "source": [
    "## Parts Of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "a11dd408-a61b-49d1-97d4-cc417283d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sushrutha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sushrutha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sushrutha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428f68f-283b-47e3-91cf-e7d95fc1b759",
   "metadata": {},
   "source": [
    "#### CC   coordinating conjunction \n",
    "#### CD   cardinal digit \n",
    "\n",
    "#### DT   determiner \n",
    "#### EX   existential there (like: “there is” … think of it like “there exists”) \n",
    "#### FW   foreign word \n",
    "#### IN   preposition/subordinating conjunction \n",
    "\n",
    "#### JJ   adjective – ‘big’ \n",
    "#### JJR  adjective, comparative – ‘bigger’ \n",
    "#### JJS  adjective, superlative – ‘biggest’ \n",
    "\n",
    "#### LS   list marker 1) \n",
    "#### MD   modal – could, will \n",
    "\n",
    "#### NN   noun, singular ‘- desk’ \n",
    "#### NNS  noun plural – ‘desks’ \n",
    "#### NNP  proper noun, singular – ‘Harrison’ \n",
    "#### NNPS proper noun, plural – ‘Americans’ \n",
    "\n",
    "#### PDT  predeterminer – ‘all the kids’ \n",
    "#### POS  possessive ending parent’s \n",
    "#### PRP  personal pronoun –  I, he, she \n",
    "#### PRP$ possessive pronoun – my, his, hers \n",
    "\n",
    "#### RB   adverb – very, silently, \n",
    "#### RBR  adverb, comparative – better \n",
    "#### RBS  adverb, superlative – best \n",
    "#### RP   particle – give up \n",
    "\n",
    "#### TO   to go ‘to’ the store. \n",
    "#### UH   interjection – errrrrrrrm \n",
    "\n",
    "#### VB   verb, base form – take \n",
    "#### VBD  verb, past tense – took \n",
    "#### VBG  verb, gerund/present participle – taking \n",
    "#### VBN  verb, past participle – taken \n",
    "#### VBP  verb, sing. present, non-3d – take \n",
    "#### VBZ  verb, 3rd person sing. present – takes \n",
    "\n",
    "#### WDT  wh-determiner – which \n",
    "#### WP   wh-pronoun – who, what \n",
    "#### WP$  possessive wh-pronoun, eg- whose \n",
    "#### WRB  wh-adverb, eg- where, when \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "850b34ac-613a-4f0e-af91-914bb7e7bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the sentence\n",
    "sentence = \"Taj Mahal is a beautiful Monument\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# POS tagging\n",
    "tagged_words = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "c1242c18-eef0-4de2-9e17-e2ba98df5392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taj', 'NNP'),\n",
       " ('Mahal', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('Monument', 'NN')]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499b86b-99a1-4a83-9589-2a3ef957e923",
   "metadata": {},
   "source": [
    "## Named Entity Recoginition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a0da2-2a3e-4bb5-869f-84006e753463",
   "metadata": {},
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
    "- **Person:** Gustave Eiffel  \n",
    "- **Place Or Location:** Eiffel Tower, France  \n",
    "- **Date:** 1887 to 1889  \n",
    "- **Time:** N/A  \n",
    "- **Money:** N/A  \n",
    "- **Organization:** Gustave Eiffel's company  \n",
    "- **Percent:** N/A  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "629c42bb-eb8c-4093-b7ba-9088963ac98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "838da7c4-6c9c-4386-9a38-a1b875a07332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "d50a666f-60ec-43c3-8cb5-97f3a9b59502",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "9152c169-ad2c-4414-a922-a6ba8709b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Sushrutha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Sushrutha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "1c7003b4-0358-47d8-9706-0220dcf6fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea81a5-1d3b-4836-ad02-9f8149f3fe09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
